---
title: "Map_making"
author: "Erica Robertson"
date: "2025-08-29"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This goal of this markdown is to run through multiple ways to make maps. We'll start with taking a typical raster file and plotting it with ggplot. Then we'll talk about adding points, such as sampling locations, to that layer. I'll go over different places to get different types of data, including built in packages in R such as rnaturalearth. Then I'll do a short session at the end for working with ebird trend data specifically: range maps, abundance maps, etc.

This doesn't replace taking a course in visualizing spatial data in R, but should be a jumping off point for making basic maps to include in proposals for research and whatnot. I'm also not going into the details of ggplot because, although I am proficient, there are far better resources outthere for specifics. Holden will also likely have his own ways of doing things, so I invite him to add in subsections with alternative approaches. Finally, I do almost all of my plotting with ggplot. Plotting with base R is also an option, and in some cases can be the better options, but I won't go over that here as I'm not good at it!

# Getting mapping data
There are ton's of places to find rasters online. For example, you can get weather data from places like WorldClim and AdaptWest. You can environmental data directly from the USDA, with crazy small resolution. You can also import basic data from packages within R, like rnaturalearth. Let's review how we might import some of these files.

*please note that I have a bad habit of switching between terra and raster packages when handling spatial data. If you master one of these packages, you don't really need both. I'll try and be consistent with just using raster here, but I might end up using some terra functions when I don't know the equivalent raster one.

## reading in .tif files
Alright, so you've found an awesome spatial file, lets say for elevation, that you want to use as the base of your maps. Let's try and work with it.

```{r}
library(raster)
#this is a file I downloaded from AdaptWest.
elev <- raster("data/elevation.tif")
#first thing I always do is plot the spatial file using base R, I avoid using ggplot for this because it doesn't handle higher resolution stuff as well
plot(elev)
```
So we can see that we've got a nice elevation layers for the entire of North America. We know from the meta data that the scale is in meters and that the resolution is 1km x 1km. Let's double check that.
```{r}
res(elev)
```

So, yes, 1000m x 1000m. Let's get the coordinate system too.
```{r}
crs(elev)
```
There's a lot of info here, but the important things that I look at are what datum is being used and what the unit is. Datum is the mathematical model that defines the size, shape, and orientation of the Earth for the coordiante system. In this case it's using WGS84. I don't think I've come across any data that doesn't use this system, but I imagine that it might be hard to merge or work with files that vary in this way. The unit being used here is meters, which I think is a silly unit but is also pretty common. I find that, especially when trying to plot gps coordinate data, it's helpful to get this into a projection that using degrees as the unit so they're compatible.

The process of projecting to a new coordinate system can take a while, so sometimes it's nice to crop down to roughly your area before doing this, just so it's not as slow.

```{r}
#let's figure out what our current extent it
extent(elev)
#making up a new extent, following the same order as the above output
ext.new <- c(-52000,36000,-41000,76000)
#cropping to that new extent
elev.crop <- crop(elev, ext.new)
#seeing what it looks like, luckily there's still variation in elevation haha
plot(elev.crop)

#now let's reproject that
elev.proj <- projectRaster(elev.crop, crs="+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0")
plot(elev.proj)
```
So the cropped raster reporjected really quickly. You can see that we're now working in lat/long space. This works better for my brain and for plotting gps sampling data!

### bonus, getting really nice elevation data
So, as you can see from the above raster, 1x1km2 resolution isn't great when you zoom in. It starts to look pretty coarse, which in my opinion doesn't look very professional. So here's a little bit of code that I put together to get high resolution elevation data for smaller areas.

First, we need to get our data. I'm going to putt just elevation data for the state of Colorado. This is a bit of a tedius process, and the resulting files take up a lot of data so be ready for that (you can delete them later but still have to store them for a bit!). Let's go to this website: https://apps.nationalmap.gov/downloader/

So, first I use the polygon tool to make a box around the state of Colorado. This doesn't have to be exact, it's just a way to filter out the data that is obviously not useful to you.

Then select elevation products. We want current data (although for elevation this probably doesn't matter much?) and the default is 1/3 arc-second DEM. Then we want the default of GeoTIFF. Now hit search products!

You're going to get a list of files, each named something like this: USGS 1/3 Arc Second n37w102 20210623. The numbers here, n37w102, give you an idea of which block you're looking at and they show up on the map to the right in green. You'll want to download every small box that covers the area you want elevation data for. Like I said, tedius, but worth it for a map you'll use again and again. Also, if you're planning mountain zone sampling (like I was) this level of resolution is super important.

So for me, this was 22GB of data! Ouch. I'm not uploading the actual data up to github because that's just too much, but the following code can be easily adjusted to whatever area or files you end up downloading. I keep all of the .tif files together in a folder called USGS_TIFF.

```{r}
##### Getting Elevation Layer #####
#Putting together high resolution elevation data for the BCRF working area
#Downloading .tifs off of USGS website and merging them together

#first let's see what just one of these files is like
test_raster <- raster("data/USGS_TIFF/USGS_13_n36w105_20220801.tif")
plot(test_raster)
#such good resolution!!

#now, we're going to read in all of the files together. This is made easier by the uniform naming scheme... first, making a list of all the file names
file_list <- list.files(path="data/USGS_TIFF/", pattern="*.tif", all.files=TRUE, full.names=TRUE)
#then using a lapply (which is kind of like a for loop) so apply the same function to every file in the list
allrasters <- lapply(file_list, raster)
#test plot to make sure that worked, just plotting the same file as above
plot(allrasters[[1]])

###### Aggregating Rasters ######
#this is an optional but encouraged step depending on what you need the rasters for. Because they are such high resolution, if you're doing too big of an area the files will be too big to work with efficiently and your computer might crash. The following for loop makes the same allraster stack as before but it aggregated them down (to a lower resolution) before stacking them.
#This is a time intensive procress, but it saves time down the line. If you need the full resolution then... well, good luck (or use the cluster)
allrasters <- list()

for (i in seq_along(file_list)) {
   # Process each raster
   raster <- raster(file_list[i])

   # Aggregate the raster
   aggregated_raster <- raster::aggregate(raster, 10)

   # Store the aggregated raster
   allrasters[[i]] <- aggregated_raster

   # Print the name of the file once the aggregation is done
   print(paste("Completed aggregation for:", basename(file_list[i])))
 }

###### Merging Rasters ######
#Now we're going to take each raster is the stack and merge them together into a single huge raster!
merged <- do.call(merge, allrasters)
plot(merged)
#go ahead and write this out as a new raster. This way you can deleted all the smaller files and save a little space.
writeRaster(merged, "data/raw_BCRF_working_area_elev.tif")

#you can also reproject and aggregate this again, whatever you need to do to make it work with other projects!
elev <- terra::project(merged, y="+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0")
elev.ag <- aggregate(elev, 10)

writeRaster(elev,"data/raw_working_area_elev.tif", overwrite=TRUE)
```







## getting eBird range data
So you can download all this information from eBird status and trends page. For example, here's the page for Bank Swallow: https://science.ebird.org/en/status-and-trends/species/banswa/trends-map

Under downloads I would usually grab the mean abundance raster (.tif file) for whichever season you're interested in. You can also grab the range geospatial data, but I'm not as familiar with working with that file type so I'll let Holden add in tidbits about that.

Let's go through the process for my Brown-capped Rosy Finches. I'll go ahead and download the mean abundance .tif file.
https://science.ebird.org/en/status-and-trends/species/bcrfin/downloads?week=1

```{r}
br <- raster("data/bcrfin_abundance_seasonal_breeding_mean_2023.tif")
plot(br)
```
So, as you can tell from this plot, there's a lot on this raster that I don't need at all. Actually, I just need the data for Colorado and part of Wyoming. There's a couple ways to handle this, so I'll demo two of them. Another option I won't show is just guessing what the extent is an cropping it down to that level, but that's a bit tedious.

```{r}
library(rnaturalearth)

elev <- raster("data/CO_WY_elev.tif")
us_states <- ne_states(country = "United States of America", returnclass = "sf")
BCRF.states <- us_states[us_states$name %in% c("Colorado", "Wyoming"), ]
plot(BCRF.states$geometry)

br.crop <- crop(br, BCRF.states)
plot(br.crop)
```







